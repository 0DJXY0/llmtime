{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data.small_context import get_datasets\n",
    "from data.metrics import calculate_crps\n",
    "from data.small_context import get_memorization_datasets\n",
    "\n",
    "datasets = get_memorization_datasets()\n",
    "nlls = defaultdict(dict)\n",
    "mae = defaultdict(dict)\n",
    "\n",
    "name_map = {\n",
    "    \"gp\": \"SM-GP\",\n",
    "    \"arima\": \"ARIMA\",\n",
    "    \"TCN\": \"TCN\",\n",
    "    \"N-HiTS\": \"N-HiTS\",\n",
    "    \"N-BEATS\": \"N-BEATS\",\n",
    "    \"text-davinci-003\": \"GPT-3\",\n",
    "}\n",
    "\n",
    "hue_order = ['SM-GP','N-BEATS','TCN','N-HiTS','ARIMA','GPT-3']\n",
    "palette = sns.color_palette('Dark2', len(hue_order))\n",
    "palette = palette[:2] + palette[3:] + palette[2:3]\n",
    "\n",
    "for dsname,(train,test) in datasets.items():\n",
    "    with open(f'../eval/memorization/{dsname}.pkl','rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "\n",
    "    # print(dsname)\n",
    "    # print(type(data_dict['gp']['samples']))\n",
    "\n",
    "    for model_name,preds in data_dict.items():\n",
    "        print(model_name)\n",
    "        if isinstance(preds['median'], np.ndarray):\n",
    "            preds['median'] = pd.Series(preds['median'])\n",
    "        try: \n",
    "            if 'NLL/D' not in preds:\n",
    "                continue\n",
    "            nll = preds['NLL/D']\n",
    "            if model_name=='text-davinci-003-tuned':\n",
    "                model_name='GPT3'\n",
    "            nlls[model_name][dsname] = nll\n",
    "            test_values = test.values if isinstance(test,pd.Series) else test\n",
    "\n",
    "            tmae = np.abs(test_values-preds['median'].values).mean()/np.abs(test_values).mean()\n",
    "            mae[model_name][dsname] = tmae\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "data = []\n",
    "\n",
    "for model_name in nlls:\n",
    "    for dsname in nlls[model_name]:\n",
    "        entry = {\n",
    "            \"model_name\": model_name,\n",
    "            \"dataset_name\": dsname,\n",
    "            \"nll\": nlls[model_name][dsname],\n",
    "            \"mae\": mae[model_name][dsname],\n",
    "        }\n",
    "        data.append(entry)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "result_df = pd.DataFrame(data)\n",
    "\n",
    "# drop all model_names except for 'gp', 'arima', 'TCN', 'text-davinci-003'\n",
    "# result_df = result_df[result_df['model_name'].isin(['arima', 'TCN', 'text-davinci-003'])]\n",
    "# result_df = result_df[result_df['model_name'].isin(['gp', 'arima', 'N-HiTS', 'TCN', 'text-davinci-003'])]\n",
    "\n",
    "result_df['dataset_name'].unique()\n",
    "result_df['NLL/D'] = result_df['nll']\n",
    "result_df['model_name'] = result_df['model_name'].apply(lambda x: name_map[x])\n",
    "\n",
    "n_cols = 11\n",
    "n_rows = 1\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale=1.3)\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, \n",
    "    figsize=(20, 4), constrained_layout=True, #sharex=True,\n",
    "    gridspec_kw={'width_ratios': [\n",
    "        4, 0.1, 1, 0.4, \n",
    "        4, 0.1, 1, 0.4, \n",
    "        4, 0.1, 1]} \n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "dsname_map = {\n",
    "    'IstanbulTraffic': 'Istanbul Traffic',\n",
    "    'TSMCStock': 'TSMC Stock',\n",
    "    'TurkeyPower': 'Turkey Power',\n",
    "}\n",
    "\n",
    "result_df['ll'] = -1 * result_df['nll']\n",
    "\n",
    "ax_idx = 0\n",
    "# Iterate through the datasets and plot the samples\n",
    "for idx, dsname in enumerate(datasets.keys()):\n",
    "    train,test = datasets[dsname]\n",
    "    print(dsname, ax_idx)\n",
    "    with open(f'eval/memorization/{dsname}.pkl','rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    ax = axes[ax_idx]\n",
    "    ax_idx += 1\n",
    "    if not 'text-davinci-003' in data_dict:\n",
    "        print('GPT-3 not available')\n",
    "        continue\n",
    "    samples = data_dict['text-davinci-003']['samples']\n",
    "    lower = samples.quantile(0.1,axis=0)\n",
    "    upper = samples.quantile(0.9,axis=0)\n",
    "    \n",
    "    # if dsname == \"square\":\n",
    "    #     print(np.min(train.values), np.max(train.values))\n",
    "\n",
    "    pred_color = sns.color_palette('Dark2')[2]\n",
    "    ax.plot(pd.concat([train,test]),color='k',label='Ground Truth', linewidth=2)\n",
    "    ax.fill_between(samples.iloc[0].index, lower, upper, alpha=0.5)\n",
    "    ax.plot(data_dict['text-davinci-003']['median'], color=pred_color,label='GPT-3 Median', linewidth=2)\n",
    "    #ax.plot(samples.T, alpha=0.5)\n",
    "    ax.set_title(dsname_map[dsname], fontsize=20)\n",
    "    # turn off y axis\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "\n",
    "    ax = axes[ax_idx]\n",
    "    ax_idx += 1\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    ax = axes[ax_idx]\n",
    "    ax_idx += 1\n",
    "    \n",
    "    sns.barplot(\n",
    "        x='dataset_name',\n",
    "        # x='data_type',\n",
    "        # order=['Trend', 'Periodic', 'Trend + Periodic'],\n",
    "        y='NLL/D',\n",
    "        hue='model_name', \n",
    "        hue_order=hue_order,\n",
    "        data=result_df[result_df['dataset_name'] == dsname], \n",
    "        ax=ax, \n",
    "        palette=palette,\n",
    "    )\n",
    "    ax.get_legend().remove()\n",
    "    if idx % 3 != 2:\n",
    "        ax.set_ylabel(\"\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"NLL/D\", rotation=-90, labelpad=15, fontsize=18)\n",
    "    ax.yaxis.set_label_position(\"right\")\n",
    "    ax.yaxis.tick_right()\n",
    "\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.plot(0, 0, 'vk', transform=ax.transAxes, clip_on=False, zorder=10)\n",
    "    ax.margins(x=0.2)\n",
    "    # ax.spines['bottom'].set_axisline_style(\"-|>\")\n",
    "    \n",
    "    # ax.set_yticks([])\n",
    "    # ax.set_ylim((-0.5,12))\n",
    "    # ax.set_ylim(bottom=-3, top=9)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "\n",
    "    if idx % 3 != 2:\n",
    "        ax = axes[ax_idx]\n",
    "        ax_idx += 1\n",
    "        ax.set_axis_off()\n",
    "\n",
    "# axes[0].legend(['Ground Truth','Zero shot GPT3'],loc='upper left',fontsize=8,frameon=True,framealpha=0.7)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "handles2, labels2 = axes[2].get_legend_handles_labels()\n",
    "handles += handles2\n",
    "labels += ['SM-GP','N-BEATS','TCN','N-HiTS','ARIMA','GPT-3']\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=.5)\n",
    "\n",
    "# # Remove unused subplots\n",
    "# for idx in range(n_datasets, n_rows * n_cols):\n",
    "#     fig.delaxes(axes[idx])\n",
    "\n",
    "plt.savefig('memorization.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('memorization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
    "ax.legend(\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    markerscale=1.5,\n",
    "    loc='upper left',\n",
    "    fontsize=20,\n",
    "    ncol=4\n",
    ")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "# plt.tight_layout()\n",
    "plt.savefig('memorization_legend.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monash samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from real_benchmarks import get_benchmark_test_sets\n",
    "\n",
    "benchmarks = get_benchmark_test_sets()\n",
    "# shuffle the benchmarks\n",
    "for k, v in benchmarks.items():\n",
    "    x, scaler = v # scaler is not used\n",
    "    # seed\n",
    "    np.random.seed(0)\n",
    "    x = np.random.permutation(x)\n",
    "    benchmarks[k] = x\n",
    "    \n",
    "df = pd.read_csv('eval/last_value_results.csv')\n",
    "df.sort_values(by='mae')\n",
    "\n",
    "df_paper = pd.read_csv('eval/paper_mae_raw.csv') # pdf text -> csv\n",
    "datasets = df_paper['Dataset']\n",
    "name_map = {\n",
    "    'Aus. Electricity Demand' :'australian_electricity_demand',\n",
    "    'Kaggle Weekly': 'kaggle_web_traffic_weekly',\n",
    "    'FRED-MD': 'fred_md',\n",
    "    'Saugeen River Flow': 'saugeenday',\n",
    "    \n",
    "}\n",
    "datasets = [name_map.get(d, d) for d in datasets]\n",
    "# lower case and repalce spaces with underscores\n",
    "datasets = [d.lower().replace(' ', '_') for d in datasets]\n",
    "df_paper['Dataset'] = datasets\n",
    "# remove from df_paper datasets in df_paper but not in df\n",
    "df_paper = df_paper[df_paper['Dataset'].isin(df['dataset'])]\n",
    "df_paper = df_paper.reset_index(drop=True)\n",
    "# for each dataset, add last value mae to df_paper\n",
    "for dataset in df_paper['Dataset']:\n",
    "    df_paper.loc[df_paper['Dataset'] == dataset, 'Last Value'] = df[df['dataset'] == dataset]['mae'].values[0]\n",
    "# turn '-' into np.nan\n",
    "df_paper = df_paper.replace('-', np.nan)\n",
    "# convert all values to float\n",
    "for method in df_paper.columns[1:]:\n",
    "    df_paper[method] = df_paper[method].astype(float)\n",
    "df_paper.to_csv('eval/paper_mae.csv', index=False)\n",
    "# normalize each method by dividing by last value mae\n",
    "for method in df_paper.columns[1:-1]: # skip dataset and last value\n",
    "    df_paper[method] = df_paper[method] / df_paper['Last Value']\n",
    "# sort df by minimum mae across methods\n",
    "df_paper['normalized_min'] = df_paper[df_paper.columns[1:-1]].min(axis=1)\n",
    "df_paper['normalized_median'] = df_paper[df_paper.columns[1:-1]].median(axis=1)\n",
    "df_paper = df_paper.sort_values(by='normalized_min')\n",
    "df_paper = df_paper.reset_index(drop=True)\n",
    "# save as csv\n",
    "df_paper.to_csv('eval/paper_mae_normalized.csv', index=False)\n",
    "\n",
    "predictable_datasets = df_paper.head(10)['Dataset']\n",
    "datasets = {k: benchmarks[k] for k in predictable_datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(inputs, targets, medians, ax):\n",
    "    sns.color_palette('Set1')\n",
    "    train = pd.Series(inputs, index=range(len(inputs)))\n",
    "    test = pd.Series(targets, index=range(len(inputs), len(inputs)+len(targets)))\n",
    "    medians = pd.Series(medians, index=test.index)\n",
    "    ax.plot(pd.concat([train,test]), color='C1',label='Ground Truth', linewidth=1)\n",
    "    ax.plot(medians, color='C4',label='GPT-3 Median', linewidth=1.5)\n",
    "    # ax.legend()\n",
    "    # remove all ticks\n",
    "    ax.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('grey')\n",
    "\n",
    "    \n",
    "\n",
    "def plot_pred_dataset(dataset, pred_dict, max_series=None, max_num_samples=None, show_median=True, title=None):\n",
    "    hyper = pred_dict['info']['hyper']\n",
    "    settings = hyper.settings\n",
    "    preds = pred_dict['preds']\n",
    "    medians = pred_dict['medians']\n",
    "    if max_series is None:\n",
    "        max_series = len(preds)\n",
    "    max_series = min(max_series, len(preds))\n",
    "    dataset = dataset[:max_series]\n",
    "    preds = preds[:max_series]\n",
    "    # separate inputs and targets\n",
    "    inputs = [xy[0][-hyper.max_history:] for xy in dataset]\n",
    "    targets = np.array([xy[1] for xy in dataset])\n",
    "    plot_predictions(inputs, targets, medians, title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "D = len(datasets)  # number of datasets\n",
    "N = 4  # number of series per dataset\n",
    "np.random.seed(99) # 2\n",
    "\n",
    "fig, axes = plt.subplots(D, N, figsize=(5.5*N, 4*D), dpi=100)\n",
    "\n",
    "for i, (ds_name, dataset) in enumerate(datasets.items()):\n",
    "    print(ds_name)\n",
    "    path = f'eval/{ds_name}.pkl'\n",
    "    with open(path, 'rb') as f:\n",
    "        pred_dict = pickle.load(f)\n",
    "\n",
    "    hyper = pred_dict['info']['hyper']\n",
    "    settings = hyper.settings\n",
    "    medians = pred_dict['medians']\n",
    "    \n",
    "\n",
    "    inputs = [xy[0][-hyper.max_history:] for xy in dataset]\n",
    "    targets = [xy[1] for xy in dataset]\n",
    "    # Select N series randomly\n",
    "    # sample without replacement\n",
    "    if N < len(medians):\n",
    "        indices = np.random.choice(len(medians), N, replace=False)\n",
    "    else:\n",
    "        indices = np.arange(len(medians))\n",
    "    inputs = [inputs[i] for i in indices]\n",
    "    targets = [targets[i] for i in indices]\n",
    "    medians = [medians[i] for i in indices]\n",
    "    \n",
    "\n",
    "    for j in range(len(indices)):\n",
    "        plot_predictions(inputs[j], targets[j], medians[j], axes[i, j])\n",
    "    # show title inside plot on the leftmost column\n",
    "    # put the title on the left\n",
    "    title = '  ' + ds_name.replace('_', ' ').title()\n",
    "    title = title.replace('Us', 'US')\n",
    "    axes[i, 0].set_title(title, fontsize=20, y=0.88, loc='left')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], color='C1', lw=2, label='Ground Truth'),\n",
    "                    Line2D([0], [0], color='C4', lw=2, label='GPT-3 Median')]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=2, fontsize=24, bbox_to_anchor=(0.5, -0.02))\n",
    "plt.tight_layout()\n",
    "plt.savefig('monash_examples.pdf', bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darts complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets = len(datasets)\n",
    "n_cols = 15\n",
    "n_rows = 2#(n_datasets + n_cols - 1) // n_cols\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale=1.3)\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, \n",
    "    figsize=(16, 4), constrained_layout=True, #sharex=True,\n",
    "    gridspec_kw={'width_ratios': [\n",
    "        3, 0.2, 1.0, 0.2, \n",
    "        3, 0.2, 1.0, 0.2, \n",
    "        3, 0.2, 1.0, 0.2,\n",
    "        3, 0.2, 1.0]}, \n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "name_map = {\n",
    "    \"gp\": \"SM-GP\",\n",
    "    \"arima\": \"ARIMA\",\n",
    "    \"TCN\": \"TCN\",\n",
    "    \"N-BEATS\": \"N-BEATS\",\n",
    "    \"N-HiTS\": \"N-HiTS\",\n",
    "    'text-davinci-003':'GPT-3',\n",
    "    'LLaMA7B': 'LLaMA7B',\n",
    "    'LLaMA13B': 'LLaMA13B',\n",
    "    'LLaMA30B': 'LLaMA30B', \n",
    "    'LLaMA70B': 'LLaMA70B',\n",
    "    \"llama1_7B\": \"LLaMA 7B\",\n",
    "    \"llama1_13B\": \"LLaMA 13B\",\n",
    "    \"llama1_30B\": \"LLaMA 30B\",\n",
    "    \"llama1_70B\": \"LLaMA 70B\",\n",
    "    \"llama2_7B\": \"LLaMA-2 7B\",\n",
    "    \"llama2_13B\": \"LLaMA-2 13B\",\n",
    "    \"llama2_70B\": \"LLaMA-2 70B\",\n",
    "    \"llama2_7B_chat\": \"LLaMA-2 7B (chat)\",\n",
    "    \"llama2_13B_chat\": \"LLaMA-2 13B (chat)\",\n",
    "    \"llama2_70B_chat\": \"LLaMA-2 70B (chat)\",\n",
    "}\n",
    "\n",
    "hue_order = ['SM-GP','N-BEATS','TCN','N-HiTS','ARIMA']#, 'LLaMA70B']\n",
    "hue_order += [\"LLaMA-2 70B\", 'GPT-3']\n",
    "# hue_order += [\n",
    "#     \"LLaMA 7B\", \"LLaMA-2 7B\", \"LLaMA-2 7B (chat)\",\n",
    "#     \"LLaMA 13B\", \"LLaMA-2 13B\", \"LLaMA-2 13B (chat)\",\n",
    "#     \"LLaMA 30B\", \"LLaMA 70B\", \"LLaMA-2 70B\", \"LLaMA-2 70B (chat)\"\n",
    "# ]\n",
    "nlls = defaultdict(list)\n",
    "crps = defaultdict(list)\n",
    "mae = defaultdict(list)\n",
    "datasets = get_datasets()\n",
    "for dsname,(train,test) in datasets.items():\n",
    "    # if dsname == \"SunspotsDataset\":\n",
    "    #     continue\n",
    "\n",
    "    # print(dsname)\n",
    "    with open(f'eval/small_context_tuned/{dsname}.pkl','rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    for model_name,preds in data_dict.items():\n",
    "        # print(f\"\\t{model_name}\")\n",
    "        if model_name in ['ada','babbage','curie']:\n",
    "            continue\n",
    "        if 'NLL/D' not in preds:\n",
    "            continue\n",
    "        nll = preds['NLL/D']\n",
    "        if model_name=='text-davinci-003-tuned':\n",
    "            model_name='GPT3'\n",
    "\n",
    "        if type(preds['samples']) == np.ndarray:\n",
    "            nlls[model_name].append(nll)\n",
    "            crps[model_name].append(calculate_crps(test.values,preds['samples'][:10],10))\n",
    "            tmae = np.abs(test.values-preds['median']).mean()/np.abs(test.values).mean()\n",
    "            mae[model_name].append(tmae)\n",
    "        else:\n",
    "            nlls[model_name].append(nll)\n",
    "            crps[model_name].append(calculate_crps(test.values,preds['samples'].values[:10],10))\n",
    "            tmae = np.abs(test.values-preds['median']).mean()/np.abs(test.values).mean()\n",
    "            mae[model_name].append(tmae)\n",
    "\n",
    "llama_models = [\n",
    "    \"llama1_7B\",\n",
    "    \"llama2_7B\",\n",
    "    \"llama2_7B_chat\",\n",
    "    \"llama1_13B\",\n",
    "    \"llama2_13B\",\n",
    "    \"llama2_13B_chat\",\n",
    "    \"llama1_30B\",\n",
    "    \"llama1_70B\",\n",
    "    \"llama2_70B\",\n",
    "    \"llama2_70B_chat\",\n",
    "]\n",
    "for dsname,(train,test) in datasets.items():\n",
    "    # if dsname == \"SunspotsDataset\":\n",
    "    #     continue\n",
    "\n",
    "    for model_name in llama_models:\n",
    "        if model_name in ['llama2_70B', 'llama2_70B_chat']:\n",
    "            fn = f'eval/llama_70B_sweep_sample/{model_name}/darts-{dsname}/1.0_0.9_0.99_0.3_3_,_.pkl'\n",
    "        else:\n",
    "            fn = f'eval/llama_2_results/{model_name}/darts-{dsname}/0.4_0.9_0.99_0.3_3_,_.pkl'\n",
    "        with open(fn,'rb') as f:\n",
    "            data_dict = pickle.load(f)\n",
    "\n",
    "        preds = data_dict#[model_name]\n",
    "\n",
    "        if 'NLL/D' not in preds:\n",
    "            continue\n",
    "        nll = preds['NLL/D']\n",
    "\n",
    "        if type(preds['samples']) == np.ndarray:\n",
    "            nlls[model_name].append(nll)\n",
    "            crps[model_name].append(calculate_crps(test.values,preds['samples'][:10],10))\n",
    "            tmae = np.abs(test.values-preds['median']).mean()/np.abs(test.values).mean()\n",
    "            mae[model_name].append(tmae)\n",
    "        else:\n",
    "            nlls[model_name].append(nll)\n",
    "            crps[model_name].append(calculate_crps(test.values,preds['samples'].values[:10],10))\n",
    "            tmae = np.abs(test.values-preds['median']).mean()/np.abs(test.values).mean()\n",
    "            mae[model_name].append(tmae)\n",
    "\n",
    "\n",
    "nlls = {k:np.array(v) for k,v in nlls.items()}\n",
    "crps = {k:np.array(v) for k,v in crps.items()}\n",
    "mae = {k:np.array(v) for k,v in mae.items()}\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame({'Dataset':dataset_keys,'NLL/D':v,'Type':k, 'CRPS':crps[k],'MAE':mae[k]}) for k,v in nlls.items()]\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df['Type'] = df['Type'].apply(lambda x: name_map[x])\n",
    "\n",
    "result_df = df\n",
    "result_df['ll'] = -1 * result_df['NLL/D']\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "datasets_to_plot = [\n",
    "    'AirPassengersDataset', 'AusBeerDataset', \n",
    "    'GasRateCO2Dataset', 'MonthlyMilkDataset', \n",
    "    'SunspotsDataset', 'WineDataset', \n",
    "    'WoolyDataset', 'HeartRateDataset'\n",
    "]\n",
    "hue_order = ['SM-GP','N-BEATS','TCN','N-HiTS','ARIMA',\"LLaMA-2 70B\",'GPT-3']\n",
    "\n",
    "ax_idx = 0\n",
    "# Iterate through the datasets and plot the samples\n",
    "for idx, dsname in enumerate(datasets_to_plot):\n",
    "    train,test = datasets[dsname]\n",
    "\n",
    "    with open(f'eval/small_context_tuned2/{dsname}.pkl','rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    ax = axes[ax_idx]\n",
    "    ax_idx += 1\n",
    "    if not 'text-davinci-003' in data_dict:\n",
    "        continue\n",
    "    samples = data_dict['text-davinci-003']['samples']\n",
    "    lower = samples.quantile(0.1,axis=0)\n",
    "    upper = samples.quantile(0.9,axis=0)\n",
    "    \n",
    "    # if dsname == \"square\":\n",
    "    #     print(np.min(train.values), np.max(train.values))\n",
    "\n",
    "    pred_color = sns.color_palette('Dark2')[2]\n",
    "    train_test = pd.concat([train,test])\n",
    "    # print(train_test.values.shape)\n",
    "    # print(list(train_test.iloc))\n",
    "    ax.plot(train_test, color='k',label='Ground Truth', linewidth=1)\n",
    "    ax.fill_between(samples.iloc[0].index, lower, upper, alpha=0.5)\n",
    "    ax.plot(samples.iloc[0].index, data_dict['text-davinci-003']['median'], color=pred_color,label='GPT3 Median',linewidth=1)\n",
    "    ax.set_title(dsname.replace(\"Dataset\",\"\"))\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "\n",
    "    import matplotlib.dates as mdates\n",
    "    myFmt = mdates.DateFormatter('%d')\n",
    "    ax.xaxis.set_major_formatter(myFmt)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(2))\n",
    "    # ax.set_xticklabels([0, 0, len(train_test) // 2, len(train_test)])\n",
    "\n",
    "    ax = axes[ax_idx]\n",
    "    ax_idx += 1\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    ax = axes[ax_idx]\n",
    "    ax_idx += 1\n",
    "    \n",
    "    palette = sns.color_palette('Dark2', len(hue_order))\n",
    "    palette = palette[:2] + palette[3:-1] + ['#a60355'] + palette[2:3]\n",
    "\n",
    "    sns.barplot(\n",
    "        x='Dataset',\n",
    "        # x='data_type',\n",
    "        # order=['Trend', 'Periodic', 'Trend + Periodic'],\n",
    "        y='NLL/D',\n",
    "        hue='Type', \n",
    "        hue_order=hue_order,\n",
    "        data=result_df[result_df['Dataset'] == dsname.replace(\"Dataset\",\"\")], \n",
    "        ax=ax, \n",
    "        palette=palette,\n",
    "    )\n",
    "    ax.get_legend().remove()\n",
    "    if idx % 4 != 3:\n",
    "        ax.set_ylabel(\"\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"NLL\", rotation=-90, labelpad=15, fontsize=11)\n",
    "    ax.yaxis.set_label_position(\"right\")\n",
    "    ax.yaxis.tick_right()\n",
    "\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.plot(0, 0, 'vk', transform=ax.transAxes, clip_on=False, zorder=10)\n",
    "    ax.margins(x=0.15)\n",
    "    # ax.spines['bottom'].set_axisline_style(\"-|>\")\n",
    "    \n",
    "    # ax.set_yticks([])\n",
    "    # ax.set_ylim((-0.5,12))\n",
    "    ax.set_ylim(bottom=-0.05)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "\n",
    "    if idx % 4 != 3:\n",
    "        ax = axes[ax_idx]\n",
    "        ax_idx += 1\n",
    "        ax.set_axis_off()\n",
    "\n",
    "\n",
    "# axes[0].legend(['Ground Truth','Zero shot GPT3'],loc='upper left',fontsize=8,frameon=True,framealpha=0.7)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "handles2, labels2 = axes[2].get_legend_handles_labels()\n",
    "handles += handles2\n",
    "labels += ['SM-GP','N-BEATS','TCN','N-HiTS','ARIMA',\"LLaMA-2 70B\",'GPT-3']\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=.5)\n",
    "# plt.locator_params(axis='x', nbins=10)\n",
    "\n",
    "# # Remove unused subplots\n",
    "# for idx in range(n_datasets, n_rows * n_cols):\n",
    "#     fig.delaxes(axes[idx])\n",
    "\n",
    "plt.savefig('darts_qualitative.pdf', dpi=300, bbox_inches='tight')\n",
    "# plt.savefig('darts_qualitative.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
    "ax.legend(\n",
    "    handles=handles[:2],\n",
    "    labels=labels[:2],\n",
    "    markerscale=1.5,\n",
    "    labelspacing=0.1,\n",
    "    loc='upper left',\n",
    "    fontsize=25,\n",
    "    ncol=8,\n",
    "    frameon=True\n",
    ")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('darts_qualitative_legend_1.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
    "ax.legend(\n",
    "    handles=handles[2:],\n",
    "    labels=labels[2:],\n",
    "    markerscale=1.5,\n",
    "    labelspacing=0.1,\n",
    "    loc='upper left',\n",
    "    fontsize=25,\n",
    "    ncol=8,\n",
    "    frameon=True,\n",
    "    handlelength=1., \n",
    "    columnspacing=1.\n",
    ")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('darts_qualitative_legend_2.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
